# Enterprise Policy & Compliance Assistant (RAG)

A **production-oriented foundation for an enterprise Retrieval-Augmented Generation (RAG) system**, focused on **robust document ingestion, preprocessing, and chunking of policy and compliance documents**.

This project addresses the **most critical and error-prone part of RAG systems**: converting unstructured enterprise PDFs into **clean, structured, metadata-rich text units** that are reliable for downstream semantic retrieval and AI-based reasoning.

---

##  Project Objective

Enterprise policies, HR handbooks, and compliance documents are typically stored as long, unstructured PDFs.
These documents are difficult to search, audit, or reason over programmatically.

This system lays the **core data pipeline** required to:

* Parse complex enterprise PDFs
* Normalize noisy text
* Split content into semantically meaningful chunks
* Preserve metadata for traceability and citations
* Prepare documents for vector-based retrieval and LLM grounding

---

## âœ… Implemented Capabilities

### ğŸ“„ PDF Document Ingestion

* Supports ingestion of enterprise PDFs (HR, policy, compliance)
* Handles multi-page documents
* Robust text extraction resilient to:

  * Headers and footers
  * Page breaks
  * Formatting noise

### ğŸ§¹ Text Cleaning & Normalization

* Removes non-informative artifacts
* Normalizes whitespace and line breaks
* Produces clean, retrieval-ready text

### âœ‚ï¸ Intelligent Chunking (RAG-Optimized)

* Recursive chunking strategy
* Configurable:

  * Chunk size
  * Overlap
* Ensures semantic coherence across chunks

### ğŸ·ï¸ Metadata Preservation

Each chunk is enriched with:

* Source document name
* Page number(s)
* Chunk index

This metadata is essential for:

* Future citation generation
* Compliance audits
* Explainable AI outputs

---

## ğŸ—ï¸ Current System Architecture

```
PDF Documents
      â”‚
      â–¼
PDF Loader
      â”‚
      â–¼
Text Cleaning & Normalization
      â”‚
      â–¼
Recursive Chunking Engine
      â”‚
      â–¼
Structured Text Chunks + Metadata
```

The output of this pipeline is **vector-store ready** and designed for seamless integration with embedding models and retrieval engines.

---

## ğŸ“ Repository Structure

```text
enterprise-policy-rag/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ pdf_loader.py      # PDF parsing & text extraction
â”‚   â”‚   â”œâ”€â”€ chunker.py         # Recursive chunking logic
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py          # Centralized configuration
â”‚   â”‚   â”œâ”€â”€ logger.py          # Logging setup
â”‚   â”‚   â””â”€â”€ constants.py
â”‚   â”‚
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_pdfs/              # Original enterprise PDFs
â”‚   â””â”€â”€ processed/             # Cleaned & chunked outputs
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_ingestion.py      # Unit tests for ingestion pipeline
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## ğŸ› ï¸ Tech Stack

| Layer         | Technology                   |
| ------------- | ---------------------------- |
| Language      | Python 3.10                  |
| PDF Parsing   | PyMuPDF / pdfplumber         |
| Chunking      | Recursive Character Splitter |
| Configuration | Pydantic                     |
| Logging       | Python logging               |
| Testing       | Pytest                       |

---

## ğŸ§  Design Rationale

### Why Focus Heavily on Ingestion?

In production RAG systems:

* **70â€“80% of failures originate from bad document preprocessing**
* Poor chunking leads to hallucinations and irrelevant answers
* Metadata loss breaks citation and compliance guarantees

This project prioritizes **data quality and traceability** over premature model integration.

### Why Recursive Chunking?

* Preserves semantic meaning
* Avoids sentence truncation
* Produces retrieval-friendly chunk boundaries

---

## ğŸ§ª Quality Assurance

* Unit tests validating:

  * PDF parsing correctness
  * Chunk size constraints
  * Metadata consistency
* Manual inspection of chunk distributions
* Deterministic preprocessing for reproducibility


## ğŸ”œ Planned Extensions

* Sentence Transformer embeddings
* Vector database integration (FAISS / ChromaDB)
* Semantic retrieval
* LLM-based answer generation with citations
* API and web-based interface
* Cloud deployment

---

##  License

MIT License
